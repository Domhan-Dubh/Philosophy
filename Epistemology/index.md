[Philosophy](./)

**Epistemology**

How do we know what we claim to know?

The recent advancements in Machine Learning (ML) can teach us a good lesson about Episemology. ML systems are able to "know" things about us, for instance. A classic and surprising example was an ML system for a big chain store that starting sending a young teen-aged girl advertisments for pregnancy products in the mail. When her father saw this he drove to the store and yelled at the manager for trying to push pregnancy on his young teen. He later drove back to the store to appologize because it turns out his daughter told him she was actually pregnant. The ML system "knew" that she was pregnant even before her father. 

Based on this, can we really say that the ML system "knew" the young woman was pregnant? Luckily, we can see everyting the ML system "knows" just by looking at it's code and database. But you'll never find a yes or no in one of these systems. Everything is "known" to some degree of certainty or probability. This is a much more mature and accurate way of looking at knowledge in general. Most students of science will tell you that it is hubris to think you can know something absolutely. That's why the highest episemological classification in science is a [Theory](./Glossary/Theory) and not a [Truth](./Glossary/Truth).

It seems ironic that a binary system seems to have a better understanding of Epistemology than most intelligent humans. Imagine a world where every single statement claiming some knowledge was qualified with a statement of reasonable doubt; a world where arguments were never fought in absolutes but in probability. Instead of, "How can you believe something so blatently false?" people would say "I disagree with the weight you give to that evidence and I feel you should give more weight to this evidence."
